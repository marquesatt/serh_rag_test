# Arquitetura TÃ©cnica: RAG Profissional com Busca SemÃ¢ntica

**Data:** Fevereiro 2026  
**VersÃ£o:** 1.0.0  
**Autor:** Sistema SERH Virtual  

---

## ğŸ“– SumÃ¡rio Executivo

Este documento detalha a implementaÃ§Ã£o de um **Retrieval Augmented Generation (RAG)** de nÃ­vel production usando:

- **Embeddings SemÃ¢nticos**: RepresentaÃ§Ã£o vetorial dos documentos via Gemini Embedding API
- **Ãndice Vetorial PrÃ©-computado**: 166 documentos Ã— 768 dimensÃµes (~2.89MB)
- **Busca por Similaridade Cosseno**: O(n) com complexidade linear, ideal para corpus pequeno-mÃ©dio
- **Gemini 3 Flash**: LLM para raciocÃ­nio final e geraÃ§Ã£o de resposta

---

## 1. Arquitetura em Camadas

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         FRONTEND (React + TypeScript)               â”‚
â”‚  - Chat UI (App.tsx, ChatMessage.tsx)              â”‚
â”‚  - Gerencia estado de mensagens                    â”‚
â”‚  - Streaming de resposta                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         GEMINI SERVICE (geminiService.ts)           â”‚
â”‚  - IntegraÃ§Ã£o com Gemini Chat API                  â”‚
â”‚  - System prompt + context injection               â”‚
â”‚  - Streaming de resposta                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      VECTOR INDEX SERVICE (vectorIndexService.ts)  â”‚
â”‚  - Carrega Ã­ndice vetorial prÃ©-computado           â”‚
â”‚  - Gera embedding da query em runtime              â”‚
â”‚  - Busca semÃ¢ntica (similarity search)             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    VECTOR INDEX (public/vectorIndex.json)          â”‚
â”‚  - 166 documentos com embeddings 768D              â”‚
â”‚  - PrÃ©-computado em build time                     â”‚
â”‚  - Armazenado em CDN Vercel                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 2. RAG (Retrieval Augmented Generation)

### 2.1 PrincÃ­pio Fundamental

**Problema**: LLMs tÃªm conhecimento limitado (cutoff) e alucinam quando perguntados sobre dados especÃ­ficos.

**SoluÃ§Ã£o**: Injetar contexto relevante ANTES de fazer o LLM raciocinar.

### 2.2 Pipeline RAG

```
USER QUERY
    â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                                     â”‚
    â–¼                                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   RETRIEVAL â”‚                   â”‚ AUGMENTATION â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤                   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1. Embeddingâ”‚                   â”‚ 1. Concat    â”‚
â”‚    Query    â”‚                   â”‚    Query +   â”‚
â”‚ 2. Search  â”‚                   â”‚    Context   â”‚
â”‚    Index   â”‚                   â”‚ 2. Format    â”‚
â”‚ 3. Get     â”‚                   â”‚    Prompt    â”‚
â”‚    Top-K   â”‚                   â”‚ 3. System    â”‚
â”‚    Docs    â”‚                   â”‚    Instructions
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚                                     â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚   GENERATION     â”‚
              â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
              â”‚ Gemini 3 Flash   â”‚
              â”‚ raciocina com    â”‚
              â”‚ contexto injetadoâ”‚
              â”‚ Gera resposta    â”‚
              â”‚ Stream chunks    â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â–¼
                 USER RESPOSTA
```

---

## 3. Embedding SemÃ¢ntico

### 3.1 O que Ã© Embedding?

**Embedding**: RepresentaÃ§Ã£o vetorial denso de um texto em espaÃ§o contÃ­nuo.

```
Texto: "AuxÃ­lio Moradia para magistrados"
                â†“
        [0.234, -0.156, 0.891, ..., 0.445]  â† 768 dimensÃµes
                â†“
        EspaÃ§o vetorial (768D)
```

### 3.2 EspaÃ§o SemÃ¢ntico

Textos semanticamente similares estÃ£o **prÃ³ximos no espaÃ§o vetorial**:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  EspaÃ§o Vetorial 768-dimensional     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                      â”‚
â”‚  "AuxÃ­lio Moradia"  â—                â”‚
â”‚                     â•±â•²               â”‚
â”‚                    â•±  â•²              â”‚
â”‚  "BenefÃ­cio Moradia" â—              â”‚
â”‚                      â•²  â•±            â”‚
â”‚                       â•²â•±             â”‚
â”‚           "Teletrabalho" â—           â”‚
â”‚                                      â”‚
â”‚   â† Similares          Diferentes â†’  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

DistÃ¢ncia: Pequena (similares)
DistÃ¢ncia: Grande (diferentes)
```

### 3.3 Gemini Embedding API

**Modelo**: `embedding-001`  
**DimensÃ£o**: 768  
**MÃ©todo**: Transformer BERT-based  

```typescript
// GeraÃ§Ã£o de embedding em build time
const response = await fetch('...embedding-001:embedContent?key=API_KEY', {
  method: 'POST',
  body: JSON.stringify({
    model: 'models/embedding-001',
    content: {
      parts: [{ text: documento.title + documento.content }]
    }
  })
});
const embedding = response.embedding.values;  // Array de 768 floats
```

---

## 4. Ãndice Vetorial PrÃ©-computado

### 4.1 Build Time vs Runtime

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   BUILD TIME     â”‚         â”‚    RUNTIME       â”‚
â”‚ (Demorado)       â”‚         â”‚  (RÃ¡pido)        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤         â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1. Ler KB (166)  â”‚         â”‚ 1. Carregar      â”‚
â”‚ 2. Chamar API    â”‚         â”‚    Ã­ndice        â”‚
â”‚    166 Ã— 768D    â”‚         â”‚ 2. Embedding     â”‚
â”‚ 3. Salvar JSON   â”‚         â”‚    query         â”‚
â”‚ 4. Deploy        â”‚         â”‚ 3. Similarity    â”‚
â”‚                  â”‚         â”‚    search        â”‚
â”‚ â±ï¸  ~3 minutos    â”‚         â”‚ 4. Retornar      â”‚
â”‚                  â”‚         â”‚                  â”‚
â”‚ ğŸ’¾ 2.89MB        â”‚         â”‚ â±ï¸  ~200ms       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 4.2 Estrutura do vectorIndex.json

```json
{
  "version": "1.0.0",
  "generatedAt": "2026-02-03T19:08:59.671Z",
  "documentCount": 166,
  "embeddingDimension": 768,
  "entries": [
    {
      "id": "1",
      "title": "AuxÃ­lio Moradia",
      "content": "...",
      "tags": ["benefÃ­cio", "moradia", ...],
      "embedding": [0.234, -0.156, 0.891, ..., 0.445]  â† 768 valores
    },
    { ... },  // 165 mais
  ]
}
```

**Tamanho**: 166 docs Ã— 768 floats Ã— 4 bytes/float â‰ˆ 512MB raw â†’ 2.89MB comprimido (gzip)

---

## 5. Similarity Search (Busca SemÃ¢ntica)

### 5.1 Cosine Similarity

**FÃ³rmula**:

$$\text{similarity}(A, B) = \frac{A \cdot B}{\|A\| \|B\|} = \frac{\sum_{i=1}^{n} a_i b_i}{\sqrt{\sum_{i=1}^{n} a_i^2} \sqrt{\sum_{i=1}^{n} b_i^2}}$$

**Range**: `[-1, 1]`
- `1.0` = idÃªnticos
- `0.5` = similares
- `0.0` = ortogonais (sem relaÃ§Ã£o)
- `-1.0` = opostos

### 5.2 ImplementaÃ§Ã£o

```typescript
function cosineSimilarity(vec1: number[], vec2: number[]): number {
  let dotProduct = 0, norm1 = 0, norm2 = 0;

  // O(n) onde n = 768
  for (let i = 0; i < vec1.length; i++) {
    dotProduct += vec1[i] * vec2[i];      // âˆ‘(ai * bi)
    norm1 += vec1[i] * vec1[i];            // âˆ‘(aiÂ²)
    norm2 += vec2[i] * vec2[i];            // âˆ‘(biÂ²)
  }

  const denominator = Math.sqrt(norm1) * Math.sqrt(norm2);
  return denominator === 0 ? 0 : dotProduct / denominator;
}
```

### 5.3 Fluxo de Busca

```
USER QUERY: "Como solicitar auxÃ­lio moradia?"
            â”‚
            â–¼
    1. EMBEDDING (Runtime)
    â”œâ”€ Chamar Gemini: "Como solicitar..."
    â””â”€ Gera: [0.145, 0.234, ..., 0.678]  â† query embedding
            â”‚
            â–¼
    2. SIMILARITY SEARCH O(n)
    â”œâ”€ Para cada doc no Ã­ndice (166):
    â”‚  â”œâ”€ cosineSimilarity(query, doc)
    â”‚  â””â”€ score = 0.85
    â”œâ”€ "AuxÃ­lio Moradia": 0.92 âœ“ TOP-1
    â”œâ”€ "BenefÃ­cio SaÃºde": 0.34
    â”œâ”€ "Teletrabalho": 0.12
    â”‚  ...
    â””â”€ TOP-5 Docs (k=5)
            â”‚
            â–¼
    3. RETORNAR TOP-K
    â”œâ”€ [Doc1 (0.92), Doc2 (0.88), Doc3 (0.76), ...]
    â””â”€ Ordem por score descendente
```

**Complexidade**: O(nÂ·d) onde n=166, d=768
- Tempo: ~200ms em cliente (JS)
- EspaÃ§o: O(nÂ·d) = ~512MB em memÃ³ria

---

## 6. GeraÃ§Ã£o de Resposta

### 6.1 InjeÃ§Ã£o de Contexto

```typescript
const systemPrompt = `
VocÃª Ã© o "Assistente Virtual SERH".

DIRETRIZES:
1. Responda APENAS baseado no CONTEXTO
2. NÃ£o invente informaÃ§Ãµes
3. Se vazio, diga que nÃ£o encontrou

CONTEXTO RECUPERADO:
${topK_documents.map((doc, i) => 
  `[${i+1}] ${doc.title}\n${doc.content}`
).join('\n\n---\n\n')}
`;

// Envia para Gemini com history
const response = await gemini.chats.create({
  model: "gemini-3-flash-preview",
  config: {
    systemInstruction: systemPrompt,
    temperature: 0.2  // Baixa criatividade = mais factual
  },
  history: conversationHistory
});
```

### 6.2 Temperature vs Criatividade

```
Temperature = 0.0  |  Temperature = 0.5  |  Temperature = 1.0
DeterminÃ­stico     |  Balanceado        |  Criativo
Factual            |  Normal            |  Alucina
"auxÃ­lio moradia   |  "auxÃ­lio moradia  |  "auxÃ­lio moradia
Ã© para..."         |  pode ser usado     |  talvez funcione
                   |  para..."           |  para viagens..."
```

**Neste projeto**: `temperature: 0.2` = muito factual (desejÃ¡vel)

---

## 7. Fluxo Completo de RequisiÃ§Ã£o

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   USER ENVIA MENSAGEM                          â”‚
â”‚              "Como solicitar auxÃ­lio moradia?"                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  App.tsx (streamChat)       â”‚
        â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
        â”‚ 1. Envia para geminiService â”‚
        â”‚ 2. Aguarda streaming        â”‚
        â”‚ 3. Acumula chunks           â”‚
        â”‚ 4. Exibe em tempo real      â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  GeminiService.streamChat()     â”‚
        â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
        â”‚ 1. Chama vectorIndexService     â”‚
        â”‚    para recuperar docs          â”‚
        â”‚ 2. Prepara system prompt        â”‚
        â”‚ 3. Chama gemini.chats.create()  â”‚
        â”‚ 4. Ativa streaming              â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ VectorIndexService.search()     â”‚
        â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
        â”‚ 1. Carrega vectorIndex.json     â”‚
        â”‚    (primeira vez = fetch)       â”‚
        â”‚ 2. Gera embedding da query      â”‚
        â”‚    (API Gemini)                 â”‚
        â”‚ 3. Calcula similarity com 166   â”‚
        â”‚    documentos                   â”‚
        â”‚ 4. Retorna TOP-5 (k=5)          â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚     Similarity Search O(nÂ·d)    â”‚
        â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
        â”‚ Para i = 0 atÃ© 166:             â”‚
        â”‚  score[i] = cosineSimilarity(   â”‚
        â”‚    queryEmbed,                  â”‚
        â”‚    docEmbed[i]                  â”‚
        â”‚  )                              â”‚
        â”‚                                 â”‚
        â”‚ sort(score) DESC                â”‚
        â”‚ return top 5                    â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  5 Documentos Similares         â”‚
        â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
        â”‚ [1] AuxÃ­lio Moradia (0.92)      â”‚
        â”‚ [2] BenefÃ­cios (0.88)           â”‚
        â”‚ [3] Requerimentos (0.76)        â”‚
        â”‚ [4] InscriÃ§Ã£o (0.71)            â”‚
        â”‚ [5] DocumentaÃ§Ã£o (0.68)         â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚    Context Injection            â”‚
        â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
        â”‚ System Prompt +                 â”‚
        â”‚ 5 Docs Concatenados             â”‚
        â”‚ User Query                      â”‚
        â”‚ Chat History                    â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   Gemini 3 Flash (Chat API)     â”‚
        â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
        â”‚ Raciocina com contexto          â”‚
        â”‚ Gera resposta token por token   â”‚
        â”‚ Envia streaming                 â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   Frontend Render               â”‚
        â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
        â”‚ Chatbot mostra resposta         â”‚
        â”‚ em tempo real                   â”‚
        â”‚ UsuÃ¡rio lÃª resposta             â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 8. OtimizaÃ§Ãµes e Performance

### 8.1 Caching em MÃºltiplos NÃ­veis

```typescript
// NÃ­vel 1: Embedding Cache (Runtime)
const embeddingCache = new Map<string, number[]>();
// Evita recompor queries repetidas

// NÃ­vel 2: Vector Index Cache (Browser)
let vectorIndex: VectorIndexFile | null = null;
// Primeira fetch = download 2.89MB
// Chamadas seguintes = use memÃ³ria

// NÃ­vel 3: CDN Cache (Vercel)
// vectorIndex.json em CDN global
// LatÃªncia: ~10-50ms (vs 2-3s sem cache)
```

### 8.2 Processamento em Lotes (Build Time)

```typescript
const BATCH_SIZE = 5;  // 5 docs por lote
const DELAY_MS = 1000; // 1s entre lotes

// Evita rate-limiting da API Gemini
// 166 docs â†’ 34 lotes â†’ ~34 segundos total
```

### 8.3 CompressÃ£o

```
Raw vectorIndex.json: ~512MB
  â†“ (gzip)
Comprimido: 2.89MB

RazÃ£o: 512/2.89 â‰ˆ 177Ã— menor
(embeddings sÃ£o muito repetitivos)
```

---

## 9. Algoritmo de RAG Detalhado

### 9.1 PseudocÃ³digo Completo

```python
def rag_response(user_query, conversation_history):
    """
    RAG Pipeline End-to-End
    
    Args:
        user_query: "Como solicitar auxÃ­lio moradia?"
        conversation_history: [...previous messages...]
    
    Returns:
        response_stream: Iterator de chunks de texto
    """
    
    # FASE 1: RETRIEVAL
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # Gera embedding da query
    query_embedding = gemini_embedding_api(user_query)
    # Shape: (768,)
    # Exemplo: [0.145, -0.234, 0.891, ..., 0.445]
    
    # Carrega Ã­ndice vetorial
    vector_index = load_vector_index()  # 166 docs Ã— 768D
    
    # Busca semÃ¢ntica O(nÂ·d)
    scores = []
    for doc in vector_index.entries:
        score = cosine_similarity(query_embedding, doc.embedding)
        scores.append((doc, score))
    
    # Top-K retrieval
    top_k_docs = sorted(scores, key=lambda x: x[1], reverse=True)[:5]
    # [(Doc1, 0.92), (Doc2, 0.88), (Doc3, 0.76), ...]
    
    # FASE 2: AUGMENTATION
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # Prepara contexto
    context = ""
    for i, (doc, score) in enumerate(top_k_docs):
        context += f"[{i+1}] {doc.title}\n{doc.content}\n\n---\n\n"
    
    # Monta prompt
    system_prompt = f"""
    VocÃª Ã© o "Assistente Virtual SERH".
    
    DIRETRIZES:
    1. Responda APENAS baseado no CONTEXTO
    2. NÃ£o invente informaÃ§Ãµes
    
    CONTEXTO:
    {context}
    """
    
    # FASE 3: GENERATION
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # Chamada Gemini com streaming
    response_stream = gemini.chats.create(
        model="gemini-3-flash-preview",
        config={
            "systemInstruction": system_prompt,
            "temperature": 0.2  # Factual
        },
        history=conversation_history,
        message=user_query
    )
    
    # Retorna stream para UI
    for chunk in response_stream:
        yield chunk  # Streaming em tempo real
```

### 9.2 AnÃ¡lise Computacional

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           ANÃLISE DE COMPLEXIDADE                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                     â”‚
â”‚ RETRIEVAL PHASE:                                    â”‚
â”‚ â”œâ”€ Embedding query: O(1) chamada API â†’ ~100ms     â”‚
â”‚ â”œâ”€ Load vectorIndex: O(1) primeira vez â†’ ~50ms    â”‚
â”‚ â”œâ”€ Similarity search: O(nÂ·d) = O(166Â·768)          â”‚
â”‚ â”‚  = 127.488 ops/query â†’ ~50ms JS/CPU             â”‚
â”‚ â”œâ”€ Top-K sort: O(n log k) = O(166 Â· log 5)        â”‚
â”‚ â”‚  = ~600 ops â†’ <1ms                              â”‚
â”‚ â””â”€ Total: ~150-200ms                               â”‚
â”‚                                                     â”‚
â”‚ GENERATION PHASE:                                   â”‚
â”‚ â”œâ”€ Gemini API call: ~500-2000ms (rede)            â”‚
â”‚ â”œâ”€ Token generation: ~50-100ms por token            â”‚
â”‚ â””â”€ Total: ~1-5 segundos de espera                  â”‚
â”‚                                                     â”‚
â”‚ TOTAL END-TO-END: ~1-5 segundos                    â”‚
â”‚                                                     â”‚
â”‚ BOTTLENECK: Gemini API latency (nÃ£o RAG)           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 10. Escalabilidade e LimitaÃ§Ãµes

### 10.1 Escalabilidade Vertical (+ documentos)

```
Documentos | Tamanho Index | Busca O(nÂ·d) | Tempo Busca
    100   |    1.74MB     |    ~77k ops  |   ~25ms
    500   |    8.70MB     |    ~384k ops |   ~100ms
   1000   |   17.4MB      |    ~768k ops |   ~250ms
   5000   |   87MB        |    ~3.8M ops |   ~1000ms
  10000   |   174MB       |    ~7.7M ops |   ~2000ms
```

**ConclusÃ£o**: 
- âœ… Para <5000 docs: Similarity search Ã© negligenciÃ¡vel
- âš ï¸ Para >10000 docs: Considere Ã­ndices mais eficientes (HNSW, IVF)

### 10.2 Alternativas para Grande Escala

```
Tamanho Corpus    | SoluÃ§Ã£o Recomendada
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  < 5000 docs    â”‚ Similarity search linear (atual)
                 â”‚ Custo: $0-50/mÃªs
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  5000-100k      â”‚ Approximate Nearest Neighbors
  docs           â”‚ Algoritmos: HNSW, IVF, PQ
                 â”‚ Ferramentas: Pinecone, Weaviate
                 â”‚ Custo: $50-500/mÃªs
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  > 100k docs    â”‚ Ãndices especializados
                 â”‚ Sharding, particionamento
                 â”‚ Elasticsearch + embeddings
                 â”‚ Custo: $500-5000+/mÃªs
```

---

## 11. Fluxo de Deploy

```
LOCAL DEVELOPMENT
        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ npm run build:vector-index â”‚  â† generateVectorIndex.ts
â”‚ - Ler knowledge_base.json   â”‚     (166 docs Ã— 768D)
â”‚ - Chamar Gemini API 166Ã—   â”‚  â† ~3 minutos
â”‚ - Salvar vectorIndex.json  â”‚  â† public/vectorIndex.json
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ npm run dev         â”‚  â† Testa localmente
â”‚ http://localhost:3001/  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ git add . && git commit â”‚  â† Commit changes
â”‚ git push              â”‚  â† Push para GitHub
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ vercel --prod        â”‚  â† Deploy Vercel
â”‚ - Build              â”‚  â† npm run build:vector-index
â”‚ - Upload dist/       â”‚  â† + vite build
â”‚ - Deploy             â”‚  â† publicize vectorIndex.json
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“
PRODUCTION (serh-rag-test.vercel.app)
â”œâ”€ Widget carrega da Vercel
â”œâ”€ Fetch vectorIndex.json (CDN)
â”œâ”€ Executa busca semÃ¢ntica
â””â”€ Gera resposta com Gemini
```

---

## 12. EquaÃ§Ãµes MatemÃ¡ticas Chave

### 12.1 Cosine Similarity

$$\cos(\theta) = \frac{\vec{A} \cdot \vec{B}}{|\vec{A}| |\vec{B}|} = \frac{\sum_{i=1}^{d} a_i b_i}{\sqrt{\sum_{i=1}^{d} a_i^2} \sqrt{\sum_{i=1}^{d} b_i^2}}$$

### 12.2 Norma L2 (Magnitude do vetor)

$$|\vec{A}| = \sqrt{\sum_{i=1}^{d} a_i^2}$$

### 12.3 Dot Product

$$\vec{A} \cdot \vec{B} = \sum_{i=1}^{d} a_i b_i$$

### 12.4 Embedding Similarity Space

Para queries q1, q2 similares:
$$\text{dist}(e(q_1), e(q_2)) \approx 0$$

Para queries q1, q2 diferentes:
$$\text{dist}(e(q_1), e(q_2)) \approx \text{grande}$$

Onde e(q) = embedding da query

---

## 13. Troubleshooting e Debugging

### 13.1 Query lenta?

```
â±ï¸ 1000ms+ para responder?

Checklist:
â˜ Gemini API rate limit? (limite: 1500 req/min)
â˜ Rede lenta? (use DevTools Network)
â˜ vectorIndex.json nÃ£o carregou? (check console)
â˜ Muitos documentos? (>10000 precisa Ã­ndice HNSW)

SoluÃ§Ã£o:
1. Verifica console.log em vectorIndexService.ts
2. DevTools > Network > vectorIndex.json
3. DevTools > Console > Errors
```

### 13.2 Respostas ruins?

```
âŒ Assistente nÃ£o acha informaÃ§Ã£o correta?

Problema: Top-K documentos recuperados sÃ£o ruins

DiagnÃ³stico:
1. Adiciona debug em vectorIndexService.ts:
   console.log("Top-5 scores:", 
     scored
     .sort((a,b) => b.score - a.score)
     .slice(0, 5)
     .map(s => ({
       title: s.entry.title, 
       score: s.score
     }))
   );

2. Verifica scores dos docs
3. Se todos scores < 0.3: KB nÃ£o tem conteÃºdo relevante
4. Se scores altos mas resposta ruim: problema no prompt

SoluÃ§Ã£o:
- Adicionar mais documentos relevantes
- Melhorar tags dos documentos
- Ajustar system prompt em geminiService.ts
```

---

## 14. Roadmap de Melhorias

### 14.1 Curto Prazo (1-3 meses)

- [ ] Adicionar mais documentos (500+)
- [ ] Implementar query rewriting (rephrase queries)
- [ ] Cache de respostas frecuentes
- [ ] Analytics: track queries, sucesso rate

### 14.2 MÃ©dio Prazo (3-6 meses)

- [ ] Ãndice HNSW para >10k docs
- [ ] Multi-modal search (imagens + texto)
- [ ] Feedback loop: usuÃ¡rio marca "Ãºtil/inÃºtil"
- [ ] Fine-tuning do prompt baseado em analytics

### 14.3 Longo Prazo (6+ meses)

- [ ] LLM customizado fine-tuned no domÃ­nio SERH
- [ ] IntegraÃ§Ã£o com sistemas internos (APIs)
- [ ] Suporte multilÃ­ngue (PT/EN)
- [ ] GraphRAG: relaÃ§Ãµes entre entidades

---

## 15. ReferÃªncias e Recursos

### Papers Seminal

- Chen et al. (2020): "Dense Passage Retrieval for Open-Domain Question Answering"
- Karpukhin et al. (2021): "DPR - Achieving state-of-the-art"
- Gao et al. (2023): "Retrieval-Augmented Generation for Large Language Models"

### Ferramentas e Benchmarks

- **Embeddings**: Sentence-Transformers, Gemini Embedding API
- **Vector DBs**: Pinecone, Weaviate, Milvus, Chroma
- **Frameworks**: LangChain, LlamaIndex, Haystack
- **Benchmarks**: BEIR (111 datasets), MTEB

---

## ConclusÃ£o

Este sistema implementa um **RAG production-ready** que combina:

âœ… **Embeddings semÃ¢nticos** (Gemini 768D)
âœ… **Ãndice vetorial prÃ©-computado** (zero latÃªncia)
âœ… **Busca por similaridade cosseno** (O(n) linear)
âœ… **InjeÃ§Ã£o de contexto** (retrieval-augmented)
âœ… **LLM reasoning** (Gemini 3 Flash)

**Resultado**: Assistente especializado que responde com conhecimento integrado, zero alucinaÃ§Ã£o, e latÃªncia aceitÃ¡vel para uso em produÃ§Ã£o.

---

**VersÃ£o**: 1.0.0  
**Ãšltima atualizaÃ§Ã£o**: 2026-02-03  
**Mantido por**: lucas.marques@serh
