este projeto implementa um sistema de retrieval-augmented generation (rag) projetado para operar como um chatbot determinístico e semanticamente fundamentado sobre uma base de conhecimento vetorizada e pré-indexada. ao invés de depender da memória paramétrica interna do llm (que invariavelmente tende a alucinar quando não sabe algo), todo o conhecimento de domínio foi externalizado em um corpus estruturado, vetorizado através de sentence embeddings e armazenado em um índice otimizado para busca por similaridade vetorial. tudo feito na mão, do zero, sem usar langchain, llamaindex ou qualquer lib de abstração duvidosa... so, respect =)

cada query do usuário passa por um processo rigoroso de normalização textual: remoção de stopwords, lowercasing, tokenização e encoding através de um modelo de embeddings (sentence-transformers, mpnet, ou similar). o vetor denso resultante (tipicamente 384 ou 768 dimensões) é usado para executar uma busca por similaridade no índice vetorial, faiss, annoy, hnswlib, ou implementação manual de knn com numpy, dependendo da escala. o sistema retorna os top-k documentos mais relevantes com base em cosine similarity ou dot product normalizado.

apenas o contexto recuperado é injetado no prompt do modelo generativo. não há acesso a dados fora do índice. o llm recebe um prompt estruturado explicitamente instruindo que respostas devem ser geradas estritamente a partir do contexto fornecido, sem extrapolação, sem criatividade, sem "baseado no meu conhecimento geral". isso garante grounding factual absoluto — se não tá no índice, não existe na resposta.
